{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: semanticscholar in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.10.0)\n",
      "Requirement already satisfied: tenacity in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from semanticscholar) (9.1.2)\n",
      "Requirement already satisfied: httpx in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from semanticscholar) (0.27.2)\n",
      "Requirement already satisfied: nest_asyncio in /Users/marwasulaiman/Library/Python/3.13/lib/python/site-packages (from semanticscholar) (1.6.0)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx->semanticscholar) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx->semanticscholar) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx->semanticscholar) (1.0.6)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx->semanticscholar) (3.10)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx->semanticscholar) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpcore==1.*->httpx->semanticscholar) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install semanticscholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    \"machine learning\"\n",
    "    # , \"artificial intelligence\", \"computer vision\",\n",
    "    # \"quantum mechanics\", \"cancer treatment\", \"COVID-19\", \n",
    "    # \"ancient civilizations\", \"economics\", \"psychology\", \"marketing\",\n",
    "    # \"music\", \"film\", \"literature\", \"education\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = SemanticScholar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m all_papers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Fetch papers for each topic and add to the list\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtopics\u001b[49m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching papers for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topics' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# sch = SemanticScholar()\n",
    "all_papers = []\n",
    "\n",
    "# Fetch papers for each topic and add to the list\n",
    "for topic in topics:\n",
    "    try:\n",
    "        print(f\"Fetching papers for: {topic}\")\n",
    "        response = sch.search_paper(query=topic, bulk=True)\n",
    "        \n",
    "        # Extract raw data from the response\n",
    "        raw_data = response.raw_data\n",
    "        \n",
    "        # Append the papers to the all_papers list\n",
    "        all_papers.extend(raw_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching papers for {topic}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"semantic_scholar_combined_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_papers, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"semantic_scholar_combined_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_papers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = all_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'papers_og.csv' has been created successfully.\n",
      "Author data has been saved to 'authors.csv'.\n",
      "Paper-Author relationship data has been saved to 'author_paper_relationship.csv'.\n",
      "Field data has been saved to 'fields_of_study.csv'.\n",
      "Paper-Field relationship data has been saved to 'paper_field_relationship.csv'.\n",
      "CSV files merged successfully!\n"
     ]
    }
   ],
   "source": [
    "csv_filename = \"papers_og.csv\"\n",
    "\n",
    "papers_data = []\n",
    "author_paper_relationship = []  # List to store paper-author relationship\n",
    "paper_field_relationship = []  # List to store paper-field relationship\n",
    "\n",
    "filtered_responses = []\n",
    "author_data = []  # List to store author names and IDs\n",
    "field_data = {}  # Dictionary to store unique fields and their IDs\n",
    "field_id_counter = 1  # Counter for assigning unique field IDs\n",
    "\n",
    "\n",
    "Journals_data = []\n",
    "Conferences_data = []\n",
    "Conf_editions = []\n",
    "seen_editions = set()\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write header row with exact headers you provided\n",
    "    writer.writerow([\n",
    "        \"Paper_ID\", \"DOI\", \"Title\", \"Abstract\", \"Venue\", \"publicationType\", \n",
    "        \"publication_type_2\", \"venue_id\", \"journal_name\", \"pages\", \"volume\", \"Edition_id\", \"Year\", \n",
    "        \"publicationDate\", \"Citations\", \"References\", \"Authors\", \"Author_Ids\", \n",
    "        \"URL\", \"s2FieldsOfStudys\", \"Corresponding_Author_ID\"\n",
    "    ])\n",
    "\n",
    "    # Write data rows\n",
    "    for paper in response:\n",
    "        paper_id = paper.get(\"paperId\", \"Unknown\") \n",
    "        doi = paper[\"externalIds\"].get(\"DOI\", \"Unknown\")  # Get DOI if available\n",
    "        title = paper.get(\"title\", \"Unknown\")\n",
    "        abstract = paper.get(\"abstract\", \"\")\n",
    "        venue = paper.get(\"venue\", \"Unknown\")\n",
    "        publication_type = (paper.get(\"publicationVenue\") or {}).get(\"type\", \"Unknown\")\n",
    "        venue_id = (paper.get(\"publicationVenue\") or {}).get(\"id\", \"Unknown\")\n",
    "        publication_type_2 = paper.get(\"publicationTypes\", [])\n",
    "        journal_name = (paper.get(\"journal\") or {}).get(\"name\", \"Unknown\") \n",
    "        pages = (paper.get(\"journal\") or {}).get(\"pages\", \"Unknown\") \n",
    "        volume = (paper.get(\"journal\") or {}).get(\"volume\", \"Unknown\") \n",
    "\n",
    "        year = paper.get(\"year\", \"2025\")\n",
    "        publicationDate = paper.get(\"publicationDate\", \"Unknown\")\n",
    "        citation_count = paper.get(\"citationCount\", \"Unknown\")\n",
    "        reference_count = paper.get(\"referenceCount\", \"Unknown\")\n",
    "        s2FieldsOfStudys = ', '.join(sorted({s2FieldsOfStudy.get('category', \"Unknown\") for s2FieldsOfStudy in paper.get(\"s2FieldsOfStudy\", [])}))\n",
    "\n",
    "        url = paper.get(\"url\", \"Unknown\")\n",
    "\n",
    "        # Extract authors and author IDs\n",
    "        authors = [author.get('name', 'Unknown') for author in paper.get(\"authors\", [])]\n",
    "        author_ids = [author.get('authorId', 'Unknown') for author in paper.get(\"authors\", [])]\n",
    "        if any(value == \"Unknown\" or value == \"\" for value in [doi, title, venue_id, publication_type, journal_name, year, authors, author_ids]):\n",
    "            continue\n",
    "\n",
    "        if len(authors) != len(author_ids):\n",
    "            continue\n",
    "\n",
    "\n",
    "        authors_str = \", \".join(f\"{author.get('name', 'Unknown')}\" for author in paper.get(\"authors\", []))\n",
    "        author_ids_str = \", \".join(f\"{author.get('authorId', 'Unknown')}\" for author in paper.get(\"authors\", []))\n",
    "\n",
    "        # Store authors and their IDs in the list for later use\n",
    "        for author_name, author_id in zip(authors, author_ids):\n",
    "            author_data.append({\"Author_Name\": author_name, \"Author_ID\": author_id})\n",
    "\n",
    "        # Mark the first author as the corresponding author\n",
    "        corresponding_author_id = author_ids[0] if author_ids else \"Unknown\"\n",
    "        \n",
    "        # Create the paper-author relationship for each author\n",
    "        for author_id in author_ids:\n",
    "            is_corresponding = (author_id == corresponding_author_id)\n",
    "            author_paper_relationship.append({\"DOI\": doi, \"Author_ID\": author_id, \"Corresponding\": is_corresponding})\n",
    "    \n",
    "\n",
    "        # Handle fields of study\n",
    "        fields_of_study = {s2FieldsOfStudy.get('category', \"Unknown\") for s2FieldsOfStudy in paper.get(\"s2FieldsOfStudy\", [])}\n",
    "\n",
    "        # Assign unique IDs to fields of study\n",
    "        for field in fields_of_study:\n",
    "            if field not in field_data:\n",
    "                field_data[field] = field_id_counter\n",
    "                field_id_counter += 1\n",
    "\n",
    "        # Create paper-field relationship\n",
    "        for field in fields_of_study:\n",
    "            paper_field_relationship.append({\"DOI\": doi, \"Field_ID\": field_data[field]})\n",
    "\n",
    "\n",
    "        if publication_type == \"conference\":\n",
    "            Conferences_data.append({\"ID\": venue_id, \"Name\": venue, \"url\": url})\n",
    "            # Conf_editions.append({\n",
    "            #         \"Edition_ID\": f\"{year}{venue_id}\",\n",
    "            #         \"Venue_ID\": f\"{venue_id}\",\n",
    "            #         \"Conference_Edition_Name\": f\"{year} {venue}\",\n",
    "            #         \"Year\": year\n",
    "            # })\n",
    "\n",
    "            edition_id = f\"{year}{venue_id}\"\n",
    "            if edition_id not in seen_editions:\n",
    "                Conf_editions.append({\n",
    "                    \"Edition_ID\": edition_id,\n",
    "                    \"Venue_ID\": f\"{venue_id}\",\n",
    "                    \"Conference_Edition_Name\": f\"{year} {venue}\",\n",
    "                    \"Year\": year\n",
    "                })\n",
    "                seen_editions.add(edition_id)\n",
    "                \n",
    "            # Generate synthetic editions for the past 3 years\n",
    "            # n=random.randint(2,6)\n",
    "            for i in range(1,5):\n",
    "                edition_year = int(year) - i\n",
    "\n",
    "                # Conf_editions.append({\n",
    "                #     \"Edition_ID\": f\"{edition_year}{venue_id}\",\n",
    "                #     \"Venue_ID\": f\"{venue_id}\",\n",
    "                #     \"Conference_Edition_Name\": f\"{edition_year} {venue}\",\n",
    "                #     \"Year\": edition_year\n",
    "                # })      \n",
    "\n",
    "\n",
    "\n",
    "                edition_id = f\"{edition_year}{venue_id}\"\n",
    "\n",
    "                if edition_id not in seen_editions:\n",
    "                    Conf_editions.append({\n",
    "                        \"Edition_ID\": edition_id,\n",
    "                        \"Venue_ID\": f\"{venue_id}\",\n",
    "                        \"Conference_Edition_Name\": f\"{edition_year} {venue}\",\n",
    "                        \"Year\": edition_year\n",
    "                    })\n",
    "                    seen_editions.add(edition_id)    \n",
    "\n",
    "\n",
    "                synthetic_paper_id = f\"{edition_year}{paper_id}\"\n",
    "                synthetic_doi = f\"{edition_year}{doi}\"\n",
    "                synthetic_title = f\"{edition_year} {title}\"\n",
    "                paper_edition=f\"{edition_year}{venue_id}\"\n",
    "\n",
    "                papers_data.append({\n",
    "                    \"Paper_ID\":synthetic_paper_id, \"DOI\":synthetic_doi, \"Title\":synthetic_title, \"Abstract\":abstract,\n",
    "                    \"Venue\":venue, \"publicationType\":publication_type, \n",
    "                    \"venue_id\":venue_id, \"journal_name\":journal_name, \"pages\":pages, \"volume\":volume, \n",
    "                    \"Edition_id\":paper_edition, \"Year\":edition_year, \"publicationDate\":publicationDate, \n",
    "                    \"Authors\":authors_str, \"Author_Ids\":author_ids_str, \"URL\":url, \n",
    "                    \"s2FieldsOfStudys\":s2FieldsOfStudys, \"Corresponding_Author_ID\":corresponding_author_id\n",
    "                })\n",
    "                # Add synthetic authors to author-paper relationship\n",
    "                for author_id in author_ids:\n",
    "                    author_paper_relationship.append({\"DOI\": synthetic_doi, \"Author_ID\": author_id, \"Corresponding\": author_id == corresponding_author_id})\n",
    "                for field in fields_of_study:\n",
    "                    paper_field_relationship.append({\"DOI\": synthetic_doi, \"Field_ID\": field_data[field]})\n",
    "\n",
    "        elif publication_type == \"journal\":\n",
    "            Journals_data.append({\n",
    "                \"ID\":venue_id, \n",
    "                \"Name\":(paper.get(\"publicationVenue\") or {}).get(\"name\", \"Unknown\"), \n",
    "                \"issn\":(paper.get(\"publicationVenue\") or {}).get(\"issn\", \"Unknown\"), \n",
    "                \"url\":(paper.get(\"publicationVenue\") or {}).get(\"url\", \"Unknown\")\n",
    "            })\n",
    "            paper_edition = \"N/A\"\n",
    "            for i in range(1,3):\n",
    "                journal_year=int(year) - i\n",
    "                synthetic_paper_id = f\"{journal_year}{paper_id}\"\n",
    "                synthetic_doi = f\"{journal_year}{doi}\"\n",
    "                synthetic_title = f\"{journal_year} {title}\"\n",
    "                \n",
    "                papers_data.append({\n",
    "                    \"Paper_ID\":synthetic_paper_id, \"DOI\":synthetic_doi, \"Title\":synthetic_title, \"Abstract\":abstract,\n",
    "                    \"Venue\":venue, \"publicationType\":publication_type, \n",
    "                    \"venue_id\":venue_id, \"journal_name\":journal_name, \"pages\":pages, \"volume\":volume, \n",
    "                    \"Edition_id\":paper_edition, \"Year\":journal_year, \"publicationDate\":publicationDate, \n",
    "                    \"Authors\":authors_str, \"Author_Ids\":author_ids_str, \"URL\":url, \n",
    "                    \"s2FieldsOfStudys\":s2FieldsOfStudys, \"Corresponding_Author_ID\":corresponding_author_id\n",
    "                })\n",
    "                for author_id in author_ids:\n",
    "                    author_paper_relationship.append({\"DOI\": synthetic_doi, \"Author_ID\": author_id, \"Corresponding\": author_id == corresponding_author_id})\n",
    "                for field in fields_of_study:\n",
    "                    paper_field_relationship.append({\"DOI\": synthetic_doi, \"Field_ID\": field_data[field]})\n",
    "        else:\n",
    "            paper_edition = \"unknown\"    \n",
    "        filtered_responses.append(paper)\n",
    "        writer.writerow([paper_id, doi, title, abstract, venue, publication_type, publication_type_2, venue_id, journal_name, pages, volume, paper_edition, year, publicationDate, citation_count, reference_count, authors_str, author_ids_str, url, s2FieldsOfStudys, corresponding_author_id])\n",
    "\n",
    "# Create DataFrame for author data\n",
    "author_df = pd.DataFrame(author_data).drop_duplicates()\n",
    "author_paper_df = pd.DataFrame(author_paper_relationship).drop_duplicates()\n",
    "field_df = pd.DataFrame(list(field_data.items()), columns=[\"Field_Name\", \"Field_ID\"]).drop_duplicates()\n",
    "paper_field_df = pd.DataFrame(paper_field_relationship).drop_duplicates()\n",
    "editions_df = pd.DataFrame(Conf_editions).drop_duplicates()\n",
    "conferences_df = pd.DataFrame(Conferences_data).drop_duplicates()\n",
    "journals_df = pd.DataFrame(Journals_data).drop_duplicates()\n",
    "\n",
    "papers_data_df = pd.DataFrame(papers_data).drop_duplicates()\n",
    "\n",
    "papers_data_df.to_csv(\"papers_synth.csv\", index=False)\n",
    "# Save to CSV\n",
    "journals_df.to_csv(\"journals.csv\", index=False)\n",
    "editions_df.to_csv(\"conference_editions.csv\", index=False)\n",
    "conferences_df.to_csv(\"conferences.csv\", index=False)\n",
    "author_df.to_csv(\"authors.csv\", index=False)\n",
    "author_paper_df.to_csv(\"author_paper_relationship.csv\", index=False)\n",
    "field_df.to_csv(\"fields_of_study.csv\", index=False)\n",
    "paper_field_df.to_csv(\"paper_field_relationship.csv\", index=False)\n",
    "\n",
    "print(f\"CSV file '{csv_filename}' has been created successfully.\")\n",
    "print(\"Author data has been saved to 'authors.csv'.\")\n",
    "print(\"Paper-Author relationship data has been saved to 'author_paper_relationship.csv'.\")\n",
    "print(\"Field data has been saved to 'fields_of_study.csv'.\")\n",
    "print(\"Paper-Field relationship data has been saved to 'paper_field_relationship.csv'.\")\n",
    "\n",
    "df1 = pd.read_csv(\"papers_synth.csv\")\n",
    "df2 = pd.read_csv(\"papers_og.csv\")\n",
    "\n",
    "# Merge (concatenate) them\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the merged CSV\n",
    "merged_df.to_csv(\"papers.csv\", index=False)\n",
    "\n",
    "print(\"CSV files merged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References CSV created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"papers.csv\")\n",
    "\n",
    "# Define the journal names (Replace with actual journal names)\n",
    "journal_1 = \"Algorithms\"\n",
    "journal_2 = \"Applied Sciences\"\n",
    "\n",
    "# Filter papers from the two journals\n",
    "journal_papers = df[(df[\"journal_name\"] == journal_1) | (df[\"journal_name\"] == journal_2)]\n",
    "\n",
    "# Select 5 random papers that are NOT from these journals\n",
    "random_papers = df[~df[\"DOI\"].isin(journal_papers[\"DOI\"])].sample(5, random_state=42)\n",
    "\n",
    "# Create a list to store references\n",
    "references = []\n",
    "\n",
    "# Assign references: Each random paper will reference all journal papers\n",
    "for _, ref_paper in random_papers.iterrows():\n",
    "    for _, journal_paper in journal_papers.iterrows():\n",
    "        references.append({\n",
    "            \"Paper_DOI\": journal_paper[\"DOI\"],\n",
    "            \"Reference_DOI\": ref_paper[\"DOI\"]\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "references_df = pd.DataFrame(references)\n",
    "\n",
    "# Save as CSV\n",
    "references_df.to_csv(\"references_synth.csv\", index=False)\n",
    "\n",
    "print(\"References CSV created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper=sch.get_paper('10.1109/ICCNS58795.2023.10193141')\n",
    "# # paper=sch.get_paper('00000c33779acab142af6c7a6dae8b36fac0805d')\n",
    "# print(json.dumps(paper.raw_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For references, retrieve the necessary info to store the papers as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Iterate over each DOI from the paper list (referenced papers) / try with first 500 now\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doi \u001b[38;5;129;01min\u001b[39;00m paper_ids_arr:\n\u001b[0;32m---> 24\u001b[0m     paper \u001b[38;5;241m=\u001b[39m \u001b[43msch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_paper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoi\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fetch paper details\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     ref_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(paper, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferenceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m paper \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(paper, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Ensure valid response\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/semanticscholar/SemanticScholar.py:127\u001b[0m, in \u001b[0;36mSemanticScholar.get_paper\u001b[0;34m(self, paper_id, fields)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03mPaper lookup\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m:raises: ObjectNotFoundException: if Paper ID not found.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    126\u001b[0m loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[0;32m--> 127\u001b[0m paper \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_AsyncSemanticScholar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_paper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaper_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaper_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m paper\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/selectors.py:548\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    546\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 548\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize lists for storing relationships and field data\n",
    "ref_author_paper_relationship = []  # List to store paper-author relationship\n",
    "ref_paper_field_relationship = []  # List to store paper-field relationship\n",
    "ref_author_data = []  # List to store author names and IDs\n",
    "\n",
    "paper_ids_arr = [paper[\"externalIds\"].get(\"DOI\") for paper in filtered_responses if paper[\"externalIds\"].get(\"DOI\")]\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open(\"references_og.csv\", \"w\", newline=\"\") as ref_file, open(\"referenced_papers.csv\", \"w\", newline=\"\") as details_file:\n",
    "    ref_writer = csv.writer(ref_file)\n",
    "    details_writer = csv.writer(details_file)\n",
    "\n",
    "    # Write headers for both CSV files\n",
    "    ref_writer.writerow([\"Paper_DOI\", \"Reference_DOI\"])\n",
    "    details_writer.writerow([\n",
    "       \"Paper_ID\", \"DOI\", \"Title\", \"Abstract\", \"Venue\", \"publicationType\", \n",
    "        \"publication_type_2\", \"venue_id\", \"journal_name\", \"pages\", \"volume\", \"Edition_id\", \"Year\", \n",
    "        \"publicationDate\", \"Citations\", \"References\", \"Authors\", \"Author_Ids\", \n",
    "        \"URL\", \"s2FieldsOfStudys\", \"Corresponding_Author_ID\"\n",
    "    ])\n",
    "\n",
    "    # Iterate over each DOI from the paper list (referenced papers) / try with first 500 now\n",
    "    for doi in paper_ids_arr:\n",
    "        paper = sch.get_paper(doi)  # Fetch paper details\n",
    "        ref_count = getattr(paper, \"referenceCount\", \"Unknown\")\n",
    "\n",
    "        if paper and hasattr(paper, \"references\"):  # Ensure valid response\n",
    "            if ref_count < 1:\n",
    "                n = 0  # No references available\n",
    "            elif ref_count < 5:\n",
    "                n = random.randint(1, ref_count)\n",
    "            elif 5 < ref_count < 10:\n",
    "                n = random.randint(5, 6)\n",
    "            else:  # ref_count >= 10\n",
    "                n = random.randint(7, 10)\n",
    "\n",
    "            top_references = paper.references[:n]  # Get top n references\n",
    "\n",
    "            for reference in top_references:\n",
    "                ref_doi = reference.externalIds[\"DOI\"] if reference.externalIds and \"DOI\" in reference.externalIds else \"Unknown\"\n",
    "                if ref_doi == \"Unknown\":\n",
    "                    continue  # Skip if DOI is unknown\n",
    "\n",
    "                # Extract reference details\n",
    "                ref_paper_id = getattr(reference, \"paperId\", \"Unknown\")\n",
    "                ref_title = getattr(reference, \"title\", \"Unknown\")\n",
    "                ref_abstract = getattr(reference, \"abstract\", \"\")\n",
    "                ref_venue = getattr(reference, \"venue\", \"Unknown\")\n",
    "                ref_publication_type = getattr(getattr(reference, \"publicationVenue\", {}), \"type\", \"Unknown\")\n",
    "                ref_venue_id = getattr(getattr(reference, \"publicationVenue\", {}), \"id\", \"Unknown\")\n",
    "\n",
    "                ref_publication_type_2 = getattr(reference, \"publicationTypes\", [])\n",
    "                ref_journal_name = getattr(getattr(reference, \"journal\", {}), \"name\", \"Unknown\")\n",
    "                ref_pages = getattr(getattr(reference, \"journal\", {}), \"pages\", \"Unknown\")\n",
    "                ref_volume = getattr(getattr(reference, \"journal\", {}), \"volume\", \"Unknown\")\n",
    "                ref_year = getattr(reference, \"year\", \"Unknown\")\n",
    "                ref_publication_date = getattr(reference, \"publicationDate\", \"Unknown\")\n",
    "                ref_citation_count = getattr(reference, \"citationCount\", \"Unknown\")\n",
    "                ref_reference_count = getattr(reference, \"referenceCount\", \"Unknown\")\n",
    "                ref_s2_fields_of_study = ', '.join(sorted({s2fs.get('category', \"Unknown\") for s2fs in getattr(reference, \"s2FieldsOfStudy\", [])}))\n",
    "                \n",
    "                # Format authors\n",
    "                ref_authors = [str(getattr(author, 'name', 'Unknown')) for author in getattr(reference, \"authors\", [])]\n",
    "                ref_authors_ids = [str(getattr(author, 'authorId', 'Unknown')) for author in getattr(reference, \"authors\", [])]\n",
    "\n",
    "                ref_url = getattr(reference, \"url\", \"Unknown\")\n",
    "\n",
    "                if ref_venue==\"Animal\":\n",
    "                    print(ref_paper_id)\n",
    "                    print(ref_publication_type)\n",
    "                    print('\\n')\n",
    "                \n",
    "                # Ensure authors and author IDs match in length\n",
    "                if len(ref_authors) != len(ref_authors_ids):\n",
    "                    continue\n",
    "\n",
    "                ref_corresponding_author_id = ref_authors_ids[0] if ref_authors_ids else \"Unknown\"\n",
    "\n",
    "                # Handle authors and relationships\n",
    "                for author_name, author_id in zip(ref_authors, ref_authors_ids):\n",
    "                    ref_author_data.append({\"Author_Name\": author_name, \"Author_ID\": author_id})\n",
    "                    ref_author_paper_relationship.append({\"DOI\": ref_doi, \"Author_ID\": author_id, \"Corresponding\": (author_id == ref_corresponding_author_id)})\n",
    "\n",
    "                # Handle fields of study\n",
    "                fields_of_study = {s2fs.get('category', \"Unknown\") for s2fs in getattr(reference, \"s2FieldsOfStudy\", [])}\n",
    "\n",
    "                # Assign unique IDs to fields of study\n",
    "                for field in fields_of_study:\n",
    "                    if field not in field_data:\n",
    "                        field_data[field] = field_id_counter\n",
    "                        field_id_counter += 1\n",
    "\n",
    "                # Create paper-field relationship\n",
    "                for field in fields_of_study:\n",
    "                    ref_paper_field_relationship.append({\"DOI\": ref_doi, \"Field_ID\": field_data[field]})\n",
    "\n",
    "\n",
    "                if ref_publication_type == \"journal\":\n",
    "                    Journals_data.append({\n",
    "                        \"ID\":venue_id, \n",
    "                        \"Name\":getattr(getattr(reference, \"publicationVenue\", {}), \"name\", \"Unknown\"),\n",
    "                        \"issn\":getattr(getattr(reference, \"publicationVenue\", {}), \"issn\", \"Unknown\"),\n",
    "                        \"url\":getattr(getattr(reference, \"publicationVenue\", {}), \"url\", \"Unknown\")\n",
    "                        })\n",
    "                    paper_edition = \"N/A\"\n",
    "                elif ref_publication_type == \"conference\":\n",
    "                    Conferences_data.append({\n",
    "                        \"ID\":venue_id, \n",
    "                        \"Name\":getattr(getattr(reference, \"publicationVenue\", {}), \"name\", \"Unknown\"),\n",
    "                        \"url\":getattr(getattr(reference, \"publicationVenue\", {}), \"url\", \"Unknown\")\n",
    "                        })\n",
    "                    paper_edition=f\"{ref_year}{ref_venue_id}\"\n",
    "                    # Conf_editions.append({\n",
    "                    #         \"Edition_ID\": paper_edition,\n",
    "                    #         \"Venue_ID\": ref_venue_id,\n",
    "                    #         \"Conference_Edition_Name\": f\"{ref_year} {venue}\",\n",
    "                    #         \"Year\": ref_year\n",
    "                    # })\n",
    "\n",
    "                    # edition_id = f\"{year}{venue_id}\"\n",
    "                    if paper_edition not in seen_editions:\n",
    "                        Conf_editions.append({\n",
    "                            \"Edition_ID\": paper_edition,\n",
    "                            \"Venue_ID\": f\"{ref_venue_id}\",\n",
    "                            \"Conference_Edition_Name\": f\"{ref_year} {ref_venue}\",\n",
    "                            \"Year\": ref_year\n",
    "                        })\n",
    "                        seen_editions.add(paper_edition)\n",
    "                else:\n",
    "                    paper_edition = \"Unknown\"\n",
    "\n",
    "                ref_writer.writerow([doi, ref_doi])\n",
    "\n",
    "                # Write the reference data to CSV\n",
    "                details_writer.writerow([\n",
    "                    ref_paper_id, ref_doi, ref_title, ref_abstract, ref_venue, ref_publication_type, \n",
    "                    ref_publication_type_2, ref_venue_id, ref_journal_name, ref_pages, ref_volume, paper_edition, ref_year, \n",
    "                    ref_publication_date, ref_citation_count, ref_reference_count, \n",
    "                    \", \".join(ref_authors), \", \".join(ref_authors_ids), ref_url, ref_s2_fields_of_study, \n",
    "                     ref_corresponding_author_id \n",
    "                ])\n",
    "\n",
    "# Create DataFrames for authors, paper-author, fields, and paper-field relationships\n",
    "author_df = pd.DataFrame(ref_author_data).drop_duplicates()\n",
    "author_paper_df = pd.DataFrame(ref_author_paper_relationship).drop_duplicates()\n",
    "field_df = pd.DataFrame(list(field_data.items()), columns=[\"Field_Name\", \"Field_ID\"]).drop_duplicates()\n",
    "paper_field_df = pd.DataFrame(ref_paper_field_relationship).drop_duplicates()\n",
    "editions_df = pd.DataFrame(Conf_editions).drop_duplicates()\n",
    "conferences_df = pd.DataFrame(Conferences_data).drop_duplicates()\n",
    "journals_df = pd.DataFrame(Journals_data).drop_duplicates()\n",
    "# Save to CSV\n",
    "journals_df.to_csv(\"journals.csv\", index=False)\n",
    "editions_df.to_csv(\"conference_editions.csv\", index=False)\n",
    "conferences_df.to_csv(\"conferences.csv\", index=False)\n",
    "# Save the DataFrames to CSV files\n",
    "author_df.to_csv(\"ref_authors.csv\", index=False)\n",
    "author_paper_df.to_csv(\"ref_author_paper_relationship.csv\", index=False)\n",
    "field_df.to_csv(\"fields_of_study.csv\", index=False)\n",
    "paper_field_df.to_csv(\"ref_paper_field_relationship.csv\", index=False)\n",
    "\n",
    "print(\"Data has been written to 'referenced_papers.csv' and other CSV files.\")\n",
    "print(\"Author data has been saved to 'ref_authors.csv'.\")\n",
    "print(\"Paper-Author relationship data has been saved to 'ref_author_paper_relationship.csv'.\")\n",
    "print(\"Field data has been saved to 'fields_of_study.csv'.\")\n",
    "print(\"Paper-Field relationship data has been saved to 'ref_paper_field_relationship.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated data without duplicates has been saved to 'Paper_nodes.csv'\n"
     ]
    }
   ],
   "source": [
    "# Paper nodes csv \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "papers_df = pd.read_csv(\"papers.csv\")\n",
    "referenced_df = pd.read_csv(\"referenced_papers.csv\")\n",
    "\n",
    "concatenated_df = pd.concat([papers_df, referenced_df], ignore_index=True)\n",
    "\n",
    "concatenated_df = concatenated_df.drop_duplicates(subset=[\"DOI\"], keep=\"first\")\n",
    "\n",
    "selected_columns = [\"DOI\", \"Title\", \"Abstract\", \"Year\", \"publicationDate\", \"Citations\", \"References\", \"URL\"]\n",
    "filtered_df = concatenated_df[selected_columns]\n",
    "\n",
    "filtered_df.loc[:, 'Abstract'] = filtered_df['Abstract'].str.replace(r'\\\"\\\"', '', regex=True)\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "filtered_df.to_csv(\"Paper_nodes.csv\", index=False)\n",
    "\n",
    "print(\"Concatenated data without duplicates has been saved to 'Paper_nodes.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated data without duplicates has been saved to 'Author_nodes.csv'\n"
     ]
    }
   ],
   "source": [
    "# Author nodes csv \n",
    "\n",
    "authors_df = pd.read_csv(\"authors.csv\")\n",
    "ref_authors_df = pd.read_csv(\"ref_authors.csv\")\n",
    "\n",
    "\n",
    "concatenated_df = pd.concat([authors_df, ref_authors_df], ignore_index=True)\n",
    "\n",
    "concatenated_df = concatenated_df.drop_duplicates(subset=[\"Author_ID\"], keep=\"first\")\n",
    "\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "concatenated_df.to_csv(\"Author_nodes.csv\", index=False)\n",
    "\n",
    "print(\"Concatenated data without duplicates has been saved to 'Author_nodes.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated data without duplicates has been saved to 'author_paper_relationship_concat.csv'\n"
     ]
    }
   ],
   "source": [
    "# Author paper relation\n",
    "\n",
    "author_paper_relationship_df = pd.read_csv(\"author_paper_relationship.csv\")\n",
    "ref_author_paper_relationship_df = pd.read_csv(\"ref_author_paper_relationship.csv\")\n",
    "\n",
    "\n",
    "concatenated_df = pd.concat([author_paper_relationship_df, ref_author_paper_relationship_df], ignore_index=True)\n",
    "\n",
    "concatenated_df = concatenated_df.drop_duplicates()\n",
    "\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "concatenated_df.to_csv(\"author_paper_relationship_concat.csv\", index=False)\n",
    "\n",
    "print(\"Concatenated data without duplicates has been saved to 'author_paper_relationship_concat.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated data without duplicates has been saved to 'paper_field_relationship_concat.csv'\n"
     ]
    }
   ],
   "source": [
    "#  paper field relation\n",
    "\n",
    "paper_field_relationship_df = pd.read_csv(\"paper_field_relationship.csv\")\n",
    "ref_paper_field_relationship_df = pd.read_csv(\"ref_paper_field_relationship.csv\")\n",
    "\n",
    "\n",
    "concatenated_df = pd.concat([paper_field_relationship_df, ref_paper_field_relationship_df], ignore_index=True)\n",
    "\n",
    "concatenated_df = concatenated_df.drop_duplicates()\n",
    "\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "concatenated_df.to_csv(\"paper_field_relationship_concat.csv\", index=False)\n",
    "\n",
    "print(\"Concatenated data without duplicates has been saved to 'paper_field_relationship_concat.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged successfully!\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"references_og.csv\")\n",
    "df2 = pd.read_csv(\"references_synth.csv\")\n",
    "\n",
    "# Merge (concatenate) them\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the merged CSV\n",
    "merged_df.to_csv(\"references.csv\", index=False)\n",
    "\n",
    "print(\"CSV files merged successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reviews, select 3 random authors as reviewers for each paper with a condition that none of them is an author of the paper. \n",
    "\n",
    "Create a csv with Paper_ID and Reviewer ID columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df = pd.read_csv(\"Paper_nodes.csv\")\n",
    "authors_df = pd.read_csv(\"Author_nodes.csv\")\n",
    "author_paper_df = pd.read_csv(\"author_paper_relationship_concat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data = []\n",
    "\n",
    "for _, paper in papers_df.iterrows():\n",
    "    paper_doi = paper[\"DOI\"]\n",
    "    # Get all authors of the paper\n",
    "    paper_authors = author_paper_df[author_paper_df[\"DOI\"] == paper_doi][\"Author_ID\"].tolist()\n",
    "\n",
    "    # Get potential reviewers (all authors who are NOT authors of this paper)\n",
    "    potential_reviewers = authors_df[~authors_df[\"Author_ID\"].isin(paper_authors)][\"Author_ID\"].tolist()\n",
    "\n",
    "    # Select 3 random reviewers \n",
    "    num_reviewers = min(3, len(potential_reviewers))\n",
    "    if num_reviewers > 0:\n",
    "        reviewers = random.sample(potential_reviewers, num_reviewers)\n",
    "        \n",
    "        for reviewer_id in reviewers:\n",
    "            reviews_data.append({\n",
    "                \"Paper_DOI\": paper_doi,\n",
    "                \"Reviewer_ID\": reviewer_id\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews CSV created successfully!\n"
     ]
    }
   ],
   "source": [
    "reviews_df = pd.DataFrame(reviews_data)\n",
    "reviews_df.to_csv(\"reviews.csv\", index=False)\n",
    "\n",
    "print(\"Reviews CSV created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
